{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8xBKYRf2YBL"
      },
      "source": [
        "## Haar Cascade Classifier Detecting Faces and Eyes\n",
        "\n",
        "* Using pretrained haar cascade classifiers to detect faces and eyes in images and video streams;\n",
        "* We use a sample image to experiment with the detection algorithm. Later, we use the webcam to capture an image to be classified;\n",
        "* Real-time facial feature detection made possible using a video stream;\n",
        "* Javascript code is utilized to capture pictures and the video stream from the webcam;\n",
        "* **Code cells dependent on Javascript are functional only in Google's Colab. They can be made functional locally through the usage of libraries such as Js2Py, that allow Javascript execution in Python notebooks.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wDhDwH2FdOAH"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Javascript, Image, clear_output\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eval_js\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from base64 import b64decode\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import display, Javascript, Image, clear_output\n",
        "from google.colab.output import eval_js"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMdguayR4fCS"
      },
      "source": [
        "## Model and Data Exploration\n",
        "\n",
        "Function `imshow` is created so that we can easily plot images using Python's Matplotlib. We cannot use OpenCV's `imshow` since it causes crashes on Colab.\n",
        "\n",
        "The function takes as argument a title for the image, the image itself and a size for it. The size argument refers to the height of the image. Width is automatically defined using the aspect ratio calculated using the original-sized image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGvPKAgUdUU2"
      },
      "outputs": [],
      "source": [
        "def imshow(image, title=\"Image\", size=8):\n",
        "    \"\"\"Convert image to RGB format and plot it in a given size\n",
        "    maintaining the aspect ratio.\n",
        "\n",
        "    Args:\n",
        "        title (str, optional): Plot title. Defaults to \"Image\".\n",
        "        image (numpy.ndarray): Image to plot.\n",
        "        size (int, optional): Image height to use in plot, in inches.\n",
        "    \"\"\"\n",
        "    h, w = image.shape[0], image.shape[1]\n",
        "    aspect_ratio = w / h\n",
        "    plt.figure(figsize=(size * aspect_ratio, size))\n",
        "    plt.title(title)\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP50OYTp5aBs"
      },
      "source": [
        "OpenCV provides pretrained haar cascade classifiers that usually perform very well. We can download them from the library's [repository at GitHub](https://github.com/opencv/opencv/tree/4.x/data/haarcascades). Anyway, they are also available at the \"data\" folder in the repository for this notebook.\n",
        "\n",
        "There are many separate models for detecting faces and parts of faces. As we are focusing on face and eye detection, we load those particular models from their respective `.xml` files.\n",
        "\n",
        "To instantiate one of these models we pass the model file to OpenCV's `CascadeClassifier` class constructor. Then, we can use the `detectMultiScale` method from the instantiated object to perform the detection. For performance reasons, we transform input images to grayscale before processing them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygXPOM8xd1GW"
      },
      "outputs": [],
      "source": [
        "# Load pretrained haar cascade classifier for face detection.\n",
        "haar_face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "image = cv2.imread('ayush.jpg')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Smaller or larger scaleFactors/minNeighbors also work for face\n",
        "# detection.\n",
        "faces = haar_face_detector.detectMultiScale(gray, scaleFactor=1.3,\n",
        "                                            minNeighbors=5)\n",
        "\n",
        "if faces is ():\n",
        "    print(\"No faces were found.\")\n",
        "else:\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
        "    imshow(image, \"Face Haar Detection\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo4xm1t7km8y"
      },
      "source": [
        "We can see that the rectangle drawn over image is precisely where his face is, even though his head is not in a front position to the camera. The more turned away from the camera is the face, the harder it is for a model to detect it.\n",
        "\n",
        "Next, we load the eye detector model and use the image found by the face detector as input to the eye detector. Eyes are harder to detect than faces, therefore we use a smaller scale factor (1.02 versus 1.3 for face detection) to get less conservative detections.\n",
        "\n",
        "**Scale factor** regulates by how much the original image is going to be downscaled at each step of the image pyramid. This rescaling is performed because the models are trained using a specific image scaling. The algorithm iterates through a couple of rescaled images searching for detectable objects.\n",
        "\n",
        "Meanwhile, we keep the **minimum number of neighbors** at 3, to avoid false positives around the eyes. This parameter forces every eye candidate portion of the image to have at least three neighboring portions belonging to eye regions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfJd2qQEf3x-"
      },
      "outputs": [],
      "source": [
        "# Load pretrained haar cascade classifier for eye detection.\n",
        "haar_eye_detector = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
        "\n",
        "image = cv2.imread('ayush.jpg')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Smaller scale factor results in a smaller detection box, meaning that\n",
        "# less data needs to be passed to the eye detector.\n",
        "faces = haar_face_detector.detectMultiScale(gray, scaleFactor=1.3,\n",
        "                                            minNeighbors=5)\n",
        "\n",
        "if faces is ():\n",
        "    print(\"No faces were found.\")\n",
        "else:\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
        "        # Crop image with slicing. Eye detection on face region only.\n",
        "        roi_color = image[y:y + h, x:x + w]\n",
        "        roi_gray = gray[y:y + h, x:x + w]\n",
        "        # Eye detector model needs a more sensitive scale factor.\n",
        "        eyes = haar_eye_detector.detectMultiScale(roi_gray, scaleFactor=1.02,\n",
        "                                                  minNeighbors=3)\n",
        "\n",
        "        if eyes is ():\n",
        "            print(\"No eyes were found.\")\n",
        "        else:\n",
        "            for (eye_x, eye_y, eye_w, eye_h) in eyes:\n",
        "                cv2.rectangle(roi_color, (eye_x, eye_y),\n",
        "                              (eye_x + eye_w, eye_y + eye_h), (0, 255, 0), 2)\n",
        "imshow(image, \"Eye and Face Haar Detection\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Uo4yU7N6jDV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOjTMeZf660F"
      },
      "source": [
        "## Webcam Imagery Detection\n",
        "\n",
        "`face_detector`, defined below, takes BGR images as input and finds faces and eyes in them.\n",
        "\n",
        "Now, we use the same models to detect eyes and faces but on images captured by the webcam of the local computer. Once again, images are grayed out before being input to models. The parameters are set differently because the image resolution is different. The image cropping technique with Numpy slicing is also replicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1N6Qo9Mvs3R"
      },
      "outputs": [],
      "source": [
        "def face_detector(image):\n",
        "    \"\"\"Detect faces and eyes in images using haar cascade classifiers\n",
        "    and draw rectangles over them.\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): Image to perform detection and draw over.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Input image with rectangles drawn over facial\n",
        "            features.\n",
        "    \"\"\"\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    faces = haar_face_detector.detectMultiScale(gray, scaleFactor=1.3,\n",
        "                                                minNeighbors=5)\n",
        "    if faces is ():\n",
        "        return image\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
        "        roi_color = image[y:y + h, x:x + w]\n",
        "        roi_gray = gray[y:y + h, x:x + w]\n",
        "        eyes = haar_eye_detector.detectMultiScale(roi_gray, scaleFactor=1.02,\n",
        "                                                  minNeighbors=3)\n",
        "\n",
        "        if eyes is ():\n",
        "            return image\n",
        "        else:\n",
        "            for (eye_x, eye_y, eye_w, eye_h) in eyes:\n",
        "                cv2.rectangle(roi_color, (eye_x, eye_y),\n",
        "                            (eye_x + eye_w, eye_y + eye_h), (0, 255, 0), 2)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekMjfYmg8eUV"
      },
      "source": [
        "Since we are working with Google's Colab, we need to use Javascript to capture images from our local camera device and send it to the cloud. `take_pic` is the function we have for that. It takes a file name to give for the captured image and the desired quality to store it.\n",
        "\n",
        "In Javascript, we create a `div` which has the video stream coming from the camera and a button to capture an image from the stream. Once the \"Capture\" button is clicked, the webcam resources are freed and the image is returned. We convert the image to base64 so that we can save it in a binary file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gPSFOOBpgLI"
      },
      "outputs": [],
      "source": [
        "def take_pic(filename='pic.jpg', quality=0.8):\n",
        "    \"\"\"Use Javascript code to get a frame from the local computer's\n",
        "    webcam once the \"Capture\" button is pressed. Bring the frame to\n",
        "    Colab and save it as a binary base64 file.\n",
        "\n",
        "    Args:\n",
        "        filename (str, optional): Name of the file to save frame into.\n",
        "            Defaults to \"pic.jpg\".\n",
        "        quality (float, optional): Image quality to transmit the frame\n",
        "            to Colab. Defaults to 0.8.\n",
        "\n",
        "    Returns:\n",
        "        str: Name of the file used to save the frame.\n",
        "    \"\"\"\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "            const div = document.createElement('div');\n",
        "            const capture = document.createElement('button');\n",
        "            capture.textContent = 'Capture';\n",
        "            div.appendChild(capture);\n",
        "\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({\n",
        "                video: true\n",
        "            });\n",
        "\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            // Resize output to fit video element.\n",
        "            google.colab.output.setIframeHeight(\n",
        "                document.documentElement.scrollHeight, true\n",
        "            );\n",
        "\n",
        "            // Wait for capture click.\n",
        "            await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    ''')\n",
        "\n",
        "    # Displays Javascript-based interface in notebook.\n",
        "    display(js)\n",
        "\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krtS77Pfq-4o"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    filename = take_pic()\n",
        "    print('Saved picture as:', filename)\n",
        "    display(Image(filename))\n",
        "except Exception as err:\n",
        "    print(str(err))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aspopgpO9z7d"
      },
      "source": [
        "With the image's file name in our hands, all we have to do is load it and send it to `face_detector`. Later, we pass the returned image, with the rectangles drawn over the facial features, directly to the `imshow` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgyJGXOkrlT4"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread(filename)\n",
        "imshow(face_detector(image), \"Eye and Face Haar Detection\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClID04ma-aTd"
      },
      "source": [
        "For last, we create two new functions that help to perform facial feature detection directly on the video stream. `json_to_image` just converts a JSON object to an OpenCV BGR image. `video_stream` performs mostly like the previous `take_pic` function, but now there is no \"Capture\" button. This time, a constant video stream is captured from the webcam and the images are constantly passed to `face_detector`.\n",
        "\n",
        "There is a \"Click here to stop the video\" button that, once pressed, closes the video stream. Resources are then freed and no frame is returned. With no frames to evaluate, the while loop ends and just the last frame evaluated is kept on display.\n",
        "\n",
        "We reduce image resolution to 640x480 to increase the overall speed of the method. However, even in lower resolutions the processing is laggy on Colab's standard machine. Each frame takes approximately 1.2 seconds to be captured, evaluated and displayed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wauzQtoFskN1"
      },
      "outputs": [],
      "source": [
        "def json_to_image(js_object):\n",
        "    \"\"\"Converts JSON object to OpenCV BGR image.\n",
        "\n",
        "    Args:\n",
        "        js_object(str): JSON object to convert to OpenCV BGR image.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: OpenCV BGR image.\n",
        "    \"\"\"\n",
        "    image_bytes = b64decode(js_object.split(',')[1])\n",
        "    # Bytes to Numpy array.\n",
        "    img_array = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "    # Numpy array to OpenCV BGR.\n",
        "    frame = cv2.imdecode(img_array, flags=1)\n",
        "\n",
        "    return frame\n",
        "\n",
        "def video_stream():\n",
        "    \"\"\"Use Javascript code to open a video stream using the local\n",
        "    computer's webcam. Transmit the captured frames to Colab until a\n",
        "    button is pressed to end the stream.\n",
        "    \"\"\"\n",
        "    js = Javascript('''\n",
        "        let video;\n",
        "        let div = null;\n",
        "        let stream;\n",
        "        let captureCanvas;\n",
        "        let imgElement;\n",
        "\n",
        "        let pendingResolve = null;\n",
        "        let shutdown = false;\n",
        "\n",
        "        // Free resources once video stream stops.\n",
        "        function removeDom() {\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            video.remove();\n",
        "            div.remove();\n",
        "            video = null;\n",
        "            div = null;\n",
        "            stream = null;\n",
        "            imgElement = null;\n",
        "            captureCanvas = null;\n",
        "        }\n",
        "\n",
        "        // Draw every frame on Colab until the stream stops.\n",
        "        function onAnimationFrame() {\n",
        "            if (!shutdown) {\n",
        "                window.requestAnimationFrame(onAnimationFrame);\n",
        "            }\n",
        "            if (pendingResolve) {\n",
        "                let result = \"\";\n",
        "                if (!shutdown) {\n",
        "                    captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "                    result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "                }\n",
        "                let lp = pendingResolve;\n",
        "                pendingResolve = null;\n",
        "                lp(result);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Create div to hold video stream and button.\n",
        "        async function createDom() {\n",
        "            if (div !== null) {\n",
        "                return stream;\n",
        "            }\n",
        "            div = document.createElement('div');\n",
        "            div.style.border = '2px solid black';\n",
        "            div.style.padding = '3px';\n",
        "            div.style.width = '100%';\n",
        "            div.style.maxWidth = '600px';\n",
        "            document.body.appendChild(div);\n",
        "\n",
        "            video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            video.width = div.clientWidth - 6;\n",
        "            video.setAttribute('playsinline', '');\n",
        "            video.onclick = () => { shutdown = true; };\n",
        "            stream = await navigator.mediaDevices.getUserMedia(\n",
        "                {video: { facingMode: \"environment\"}});\n",
        "            div.appendChild(video);\n",
        "\n",
        "            imgElement = document.createElement('img');\n",
        "            imgElement.style.position = 'absolute';\n",
        "            imgElement.style.zIndex = 1;\n",
        "            imgElement.onclick = () => { shutdown = true; };\n",
        "            div.appendChild(imgElement);\n",
        "\n",
        "            const instruction = document.createElement('div');\n",
        "            instruction.innerHTML =\n",
        "                '<span style=\"blue: red; font-weight: bold;\">' +\n",
        "                'click here to stop the video</span>';\n",
        "            div.appendChild(instruction);\n",
        "            instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "            captureCanvas = document.createElement('canvas');\n",
        "            captureCanvas.width = 640;\n",
        "            captureCanvas.height = 480;\n",
        "            window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "            return stream;\n",
        "        }\n",
        "\n",
        "        // Function to manage the whole Javascript code.\n",
        "        async function stream_frame() {\n",
        "            if (shutdown) {\n",
        "                removeDom();\n",
        "                shutdown = false;\n",
        "                return '';\n",
        "            }\n",
        "\n",
        "            stream = await createDom();\n",
        "\n",
        "            let result = await new Promise(function(resolve, reject) {\n",
        "                pendingResolve = resolve;\n",
        "            });\n",
        "            shutdown = false;\n",
        "\n",
        "            return result\n",
        "        }\n",
        "    ''')\n",
        "\n",
        "    # Displays Javascript-based interface in notebook.\n",
        "    display(js)\n",
        "\n",
        "# Start webcam stream.\n",
        "video_stream()\n",
        "\n",
        "# While the stream is not closed, perform detection.\n",
        "while True:\n",
        "    clear_output(wait=True)\n",
        "    frame_js = eval_js('stream_frame()')\n",
        "    if not frame_js:\n",
        "        break\n",
        "    img = json_to_image(frame_js)\n",
        "    imshow(face_detector(img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBGwmr7bA26f"
      },
      "source": [
        "The models work fine and they rarely give false positives. Nonetheless, if the face is turned or too far away from the camera, they fail to detect facial features. It is also important to consider that facial feature detection models usually have a hard time detecting features on bearded faces, given a lack of training samples having that feature.\n",
        "\n",
        "Lower resolutions may increase speed performance but will also make it harder for the model to work. Making the haar models have less resource-demanding parameters (scale factor set higher and mininum neighborhood set lower) does not help much, so we tuned them for better accuracy."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
